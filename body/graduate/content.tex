\chapter{绪论}
\section{背景与意义}

随着科技的进步，机器人技术近年来实现了飞速发展，其应用场景已从传统的工业制造领域，
迅速拓展至医疗、教育、军事、交通等更广阔的社会服务与民生领域。机器人技术已成为驱动
未来新兴产业发展和科技创新的关键基础，对促进国民经济增长和加强国防建设具有不可替代
的战略意义\cite{2013RobotProgress}。移动与操作是机器人需要实现的两个功能。
在各种机器人构型中，腿足式机器人因其仿生学设计，展现出卓越的环境适应性和通过能力，
能够灵活跨越障碍、稳定通过复杂路面，使其成为探险、搜救和野外作业等领域不可或缺的
理想平台\cite{孟健2015复杂地形环境四足机器人运动控制方法研究与实现}。但是在现实中，
许多任务涉及操作任务，尽管腿足式机器人可以使用腿足完成推、拉等简单的
操作任务，这种方式通常会削弱机器人的移动能力，且难以在运动过程中进行精细化的操作。
为四足机器人装配机械臂（Legged Manipulator System，简称LMS）成为了增强其作业能力的关键途径。
机械臂的加入使得四足机器人从单纯的移动平台升级为了具备操作能力的多功能系统，能够执行远远
操作仅仅使用腿足作为执行器的复杂精细的多样化任务。
例如，在电力、油气等基础设施的巡检任务中，足式机械臂可以行走在复杂的管道或崎岖地面上，
并利用机械臂搭载传感器或工具进行高精度仪表读数、阀门开关甚至简单故障的现场维修。
在生活服务与辅助照护领域，足式机械臂能够稳定的跨过家庭环境中的地毯、门槛与楼梯场景，并
利用机械臂完成拿快递、开门与打扫房间等任务。
% 在地震废墟、核泄漏场所或有毒气体环境中，履带和轮式机器人往往寸步难行，而足式机械臂
% 可以在碎石瓦砾上行走并利用机械臂移除障碍物、打开紧闭的门、递送急救物资，甚至进行
% 生命探测仪器的精确部署。它们能够深入危险地带执行传统机器人无法完成的“侦查-清理-救援”
% 一体化复杂任务，极大地提高了救援效率和安全性。

然而，四足机械臂控制系统是一个高自由度、强耦合和高度非线性的动态系统，关于该构型的全身
目标规划与全身协同控制仍然是当前研究中的关键难点。尤其在全身协同控制层面，机械臂的运动会对四足机器人
稳定性会造成显著的扰动；同时，四足机器人还必须主动的进行步态调整与位姿配合，以扩展机械臂的操作空间，
确保操作任务的成功执行。如果高效的解决这种多目标、强耦合的全身动态协调控制问题，是制约足式机械臂
从理论走向实际应用的关键瓶颈。
目前，模型预测控制（Model Predictive Control，简称MPC）是实现足式机械臂精确控制的主流方法。
不过其控制方法存在显著的局限性：首先，它必须依赖于精确且复杂的全身动力学模型，这给建模过程带来了巨大
的工程难度和不确定性；其次，MPC的计算复杂度高，难以在高度动态和实时环境（如复杂地形）中满足高频率
的控制需求；此外，在机械臂末端执行器进行复杂操作时，还需要耗费巨大的工程努力来处理和避免奇异点等问题，
以确保控制的稳定性和精度。
相比之下，近年来，深度强化学习（Deep Reinforcement Learning, 简称DRL）在机器人控制领域，
尤其是四足机器人运动控制方面，展现出强大的鲁棒性与环境适应性
\cite{gu2017deep, kalashnikov2018scalable, chen2023visual, kim2024not, nahrendra2023dreamwaq}。
DRL通过结合深度神经网络的强大拟合能力和强化学习的决策优化能力，使智能体通过与环境的大规模
并行化仿真交互，自主学习并优化控制策略。这种基于数据驱动的控制范式，显著减少了对精确物理建模的依赖，
提供了一种更加灵活、高效且鲁棒的全身控制解决方案。因此，深入研究基于深度强化学习的
四足机械臂全身运动协同控制方法，对于推动该平台在复杂任务场景中的智能化和自动化应用具有
重大的理论与实际意义。

综上所述，如何利用深度强化学习算法实现足式机械臂流畅且鲁棒的全身协同控制是本文的核心研究目标。
足式机械臂的全身协同控制问题可以被抽象为两个相互耦合的控制任务：
一是四足机器人的速度追踪运动控制；二是机械臂末端执行器的位姿追踪控制。
这两个任务是相互制约、紧密耦合的。具体而言，基座的动态运动会对机械臂的位姿跟踪
产生扰动，而机械臂的运动反过来也会对基座的稳定造成挑战。更重要的是，
为了让机械臂能够到达并稳定操作给定的目标位姿，基座必须流畅地做出主动配合行为
（如俯仰、侧倾），而这种配合行为的设计难度极高，且会对两个控制目标造成进一步的耦合影响。
针对上述挑战，本文提出一种基于深度强化学习的足式机械臂全身协同控制方法。
具体而言，我们利用机械臂的运动学模型，建立基座姿态与机械臂工作空间的可达性之间的映射关系，
并据此设计了物理可行性引导（Physical Feasibility Guidance, 简称PFG）奖励机制。
该机制通过实时判断当前躯干姿态是否能让机械臂在关节配置安全域内达到目标位姿，
从而有效约束并引导策略探索方向，最终实现了足式机械臂流畅、鲁棒的全身运动与操作协同。

\section{问题与挑战}
由于足式机械臂在物流、巡检、赈灾救援等领域展现出巨大的应用潜力，其复杂性对机器人的
控制与部署系统提出了更高的要求。基于前文对该系统特性和现有方法局限性的分析，
本文的主要研究内容聚焦于解决以下关键问题与挑战：

\textbf{足式机械臂全身协同控制：}由于MPC方法需要复杂的建模建模步骤、高昂
的计算开销，并且难以实现身体与手臂之间的自然流畅配合，这推动了研究向数据驱动的深度强化学习转型。
尽管DRL优势显著，但应用于足式机械臂的全身控制仍面临一些严峻的挑战：
\begin{itemize}
      \item 多目标耦合与次优收敛： 足式机械臂的全身控制涉及基座稳定运动和
            机械臂精确操作这两个强耦合的任务目标。DRL策略的优化依赖于奖励函数
            对两个目标的加权评估。然而，由于奖励函数的权衡设计难度极高，DRL算法难以在优化过程
            中有效平衡相互制约的任务。这导致训练出的策略容易过度倾向于某一特定任务，
            最终收敛到次优或局部最优解，严重影响了控制器最终性能。
      \item 探索效率与行为流畅性缺失： 现有的DRL协同控制方法在处理高维连续动作空间
            时，通常缺乏对系统物理可行性的先验指导，导致策略探索效率低下。已有的解耦训练
            或优势混合方法往往采用两阶段训练，训练耗时，且难以得到高度自然、流畅且具备
            高鲁棒性的全身协同行为，制约了策略的实用性。
      \item 高精度追踪的难以实现： 相比于MPC等基于模型的优化方法，DRL策略
            由于其固有的函数逼近误差和随机探索特性，在实现高精度、高收敛性的末端执行器
            位姿追踪任务时往往表现不佳。这种相对较低的追踪精确度，对于需要精细触碰、抓取
            或插入等操作任务的机械臂而言，是阻碍足式机械臂广泛应用的重要障碍。
\end{itemize}

\textbf{足式机械臂异构系统的Sim2Real部署鸿沟：}
足式机械臂系统本质上是一个异构系统：它将四足机器人与机械臂集成在一起。
这种异构性导致Sim2Real（仿真到现实）部署鸿沟被进一步放大。
\begin{itemize}
      \item 驱动器特性差异： 四足机器人通常采用大扭矩、低减速比的行星减速器电机，
            强调动态响应和功率密度；而机械臂则普遍采用高减速比、
            高精度的谐波减速器电机，强调刚度和高精度定位。
            这种显著的硬件差异导致其电机控制特性在仿真中难以被统一和精确建模。
      \item 高增益控制的仿真难度： 机械臂为了保证操作精度，需要使用较大
            的$K_p$和$K_d$增益进行控制。在仿真环境中，使用如此大的控制增益
            来精确模拟机械臂的电机控制特性极具挑战性，细微的建模误差在高增益下
            会被放大，导致在仿真中训练过程难以收敛，策略输出趋向于振荡，极大地制约
            了强化学习策略的实际部署。
\end{itemize}

\textbf{全身控制系统的故障容错与鲁棒性架构设计：}
在现实世界的任务执行中，故障障容错能力和系统鲁棒性是足式机械臂从实验室走向实
际应用的关键指标。足式机械臂的复杂性要求其能够在执行操作时，有效地应对子系统（机械臂）
的突发性故障，避免控制系统崩溃。
\begin{itemize}
      \item 功能解耦与故障隔离挑战： 尽管足式机械臂的控制目标是高度耦合的，
            但其控制主体（基座与机械臂）在控制逻辑上应实现故障隔离。
            具体而言，当机械臂因电机过热或传感器故障等问题
            导致控制失效时，控制系统不应因该子系统的故障而崩溃，
            而是需要迅速克服机械臂失控带来的扰动，保持基座的稳定运动控制，
            从而将故障机械臂安全地转移到收纳或维修位姿。
      \item 鲁棒性架构与先验知识的局限： 基于模型的传统方法（如MPC）
            通常将足式机械臂视为一个单一体、完全耦合的系统进行优化，其控制律依赖
            于模型的假设，本质上缺乏对子系统故障的内在感知和应对机制。
            虽然深度强化学习（DRL）作为一种数据驱动的方法，
            有可能通过在仿真中注入故障数据来学习相应的故障容错策略和解耦状态，
            但如何高效、系统性地构建并验证这种具备故障容错能力的DRL架构，
            仍是一个问题。
\end{itemize}

\textbf{非结构化环境中的动态全身控制与感知融合：}
足式机械臂的最终应用场景是户外、灾区或家庭等典型的非结构化环境。在这些场景中，足式机械臂不仅要克服
复杂、动态变化的崎岖地形如碎石、泥泞、楼梯，还必须使用机械臂在移动过程中进行相应操作任务。
\begin{itemize}
      \item 高动态全身协同控制的实时性与鲁棒性：在复杂地形上保持稳定移动本身就是一项
            高动态控制难题。当系统同时执行全身协同操作时，需要在极高频率的运动控制环中实时
            融合环境感知、本体状态和操作目标信息。实现基座动态稳定与机械臂操作一体化的控制系统，
            对算法的提出了极高的要求。
      \item 感知不确定性与地形适应性挑战：非结构化环境中的传感器数据不可避免地存在
            噪声、漂移和遮挡（即感知不确定性）。足式机械臂需要智能地在依赖感知的模式
            与不依赖感知的盲走模式之间进行鲁棒切换，以适应环境变化。此外，为了在复杂
            地形上扩展机械臂的工作空间或保持身体稳定，系统必须具备强大的全身协同能力，
            以在满足操作约束的同时，确保足端与地面的物理可行性接触，
            并利用机械臂完成相应操作任务。
\end{itemize}

\section{本文研究内容与贡献}

基于上述全身控制器面临的关键挑战，本文围绕基于强化学习的足式机械臂全身协同运动
控制方法、异构系统sim2real方法、物理可行域引导的奖励设计方法、与基于空间注意
力的感知融合方法展开研究。

在足式机械臂的全身控制架构方法，我们采用了PPO强化学习算法，使用了非对称性Actor-Critic架构。
在训练过程中，Actor与Critic网络接受的输入完全不一样。Critic没有任何的部署的需求，因此接受
理想无噪声的机器人状态数据，这些状态数据可以直接从仿真中获取。这种方法使得Critic估计的Value
值更加准确，从而高效的指导策略收敛。而Actor网络（即策略网络）的输入则严格受限于在真实机器人上
可获取的信息，这确保了策略的直接可部署性。实验证明采用非对称性的网络结构设计能够加速收敛，提升
训练效率与最终策略的效果。

在此基础上，为了弥补足式机械臂控制系统在仿真域现实之间的巨大鸿沟，我们采用了分离式框架设计，
结合域随机化与课程设计方法，通过对各个传感器收集的数据加入噪声和延迟，来模拟真实机器人上的
传感器数据。例如：对于机器人控制器，我们采用了双延迟缓冲器的设计来模拟控制延迟。对于机器人关节
状态数据，我们通过单延迟缓冲器与引入高斯噪声来模拟真实机器人上获得的关节状态数据。
对于地形感知数据，我们模仿elevation mapping地图更新方法，
通过内部维护部分一个局部地图，来模拟在真实机器人上获得的地形感知数据。
除此之外，机械臂与四足机器人采用的电机不同，相比四足机器人的电机，机械臂采用了谐波减速器，需要极大的
$K_p$与$K_d$增益来实现高精度的控制。在仿真训练中，策略网络的微小输出被较大的$K_p$与$K_d$
放大，导致机械臂运动振荡，这使得训练的策略难以部署在真实的机器人上。为了解决这个问题，
我们引入了虚拟力奖励引导机制，力矩奖励并非由机械臂真实的控制力矩生成，而是由一个理想pd控制器生成，
尽管该力矩并不是真正的作用在机械臂上。但是在优化过程中，这个奖励可以让策略网络学习
到PD控制器位置与力的关系，进而改变输出幅值，减轻振荡效应。

操作与移动的全身协同控制问题是足式机械臂控制中的关键难点。
在学习过程中，策略往往难以学习身体与手臂的配合行为，原因是，身体的配合会对四足机器人
的速度追踪任务产生负面影响，为了获得更好的速度追踪性能，策略往往会忽略身体的配合行为，
此时策略逐渐收敛至局部最优。
针对现有DRL方法在平衡“稳定移动”和“精确操作”两大耦合目标时容易收敛至局部最优的局限。
本文提出了基于物理可行域引导的奖励设计方法。该方法借鉴生物体在移动中进行操作的
协同原理，将显式的机器人运动学模型作为先验知识引入奖励函数。
在训练过程中，当身体状态使得操作任务可达时，该身体状态被是视为一个可行身体状态。
策略不仅仅追求任务奖励的最大化，还需要探索并保留在给定操作任务下的可行
身体状态。该方法能有效的引导了策略探索方向，使得策略收敛避开局部最优解，最终实现了足式
机械臂流畅、鲁棒的全身运动与操作协同。

此外，足式机械臂在非结构化场景中面临的传感器不确定性和故障容错挑战，我们设计了一种基于
空间注意力机制的网络模型，以实现机器人内部状态与环境感知的鲁棒融合。该模型利用空间注意
力机制动态调整网络对环境感知信息的权重，并结合门控神经网络结构，实现了对感知信息的鲁棒
处理。当外部感知数据因噪声或故障而不可用时，网络能够智能地退化为基于内部状态的无感知模式。
这种动态的鲁棒切换能力，有效增强了全身控制器的环境适应性与故障容错能力，确保了足式机械臂
在复杂地形上执行操作任务的可靠性。

\section{本文结构}
本文共有六章，结构安排如下：

第一章主要介绍了足式机械臂系统的发展背景与研究意义，明确指出了当前足式机械臂全身协同控制中的常用方法与该领域面临的技术挑战，并概述了本文的研究动机、主要内容与核心贡献。

第二章详细介绍了与足式机械臂全身控制相关的模型预测控制与强化学习理论知识，详细介绍了基于MPC的足式机械臂的全身控制方法与基于强化学习的全身控制方法，分析了当前足式机械臂全身控制领域的研究现状，评述了各自的优势与不足，从而为后续研究内容奠定基础。

\chapter{足式机械臂全身控制方法概述}

\section{引言}

四足机械臂全身控制任务被分解为两个控制任务，一个是四足机器人的运动控制，即希望
四足机器人能够在给定速度指令下稳定行走。第二是机械臂的位姿控制，给定机械臂末端
执行器的目标位姿，希望机械臂尽快的到达该目标位姿并保持稳定。在实际任务中，两者
高度耦合。比如在四足机器人站立姿态下，机械臂的工作空间受到限制，为了扩展
机械臂的工作空间，四足机器人的身体姿态也需要满足相应的要求。在学界，足式机械臂的
全身控制主要分为基于MPC的控制方法与基于强化学习的控制方法两大类。MPC 依赖于
精确的动力学模型和高效的实时优化求解器，能够显式处理复杂的动力学约束
和接触力约束，提供运动轨迹的可行性和理论保证。由于足式机械臂的接触状态（脚和地面的接触
状态，机械臂末端执行器与操作物体的接触状态）是不断切换的，因此MPC将多点接触系统建模
为切换系统，并依赖优化器进行实时的求解。对模型与优化的强依赖性导致MPC方法控制效果不过
鲁棒，尤其优化器十分耗时，难以满足实时性要求。
深度强化学习（DRL）为足式机械臂全身控制提供了一条无需精确物理建模的路径。
通过端到端学习，DRL 能够训练出高度鲁棒且生物学上协同的统一策略，有效应对
高维度动作空间和复杂动力学耦合。因此，本文基于强化学习，通过设计合理的奖励函数与网络结构，
实现流畅鲁棒的强化学习全身控制器。本章详细介绍了现有足式机械臂的主要平台，并分析了现有
平台控制方法的优点与缺点。
% 然后对强化学习相关内容进行介绍，为本文后续内容做好基础讲解。

\section{足式机械臂硬件平台}

足式机械臂系统是移动操作机器人领域的重要分支，其本质是在四足移动机器人的躯干上
集成一个或多个多自由度机械臂，从而构成具备“移动”与“操作”双重能力的复合系统。
% 与传统的轮式或履带式移动操作平台相比，四足底盘摆脱了对连续平坦地面的依赖，
% 能够适应台阶、碎石等非结构化地形，显著拓展了机器人的作业空间。
% 而相较于双足人形机器人，四足底盘拥有更大的支撑多边形和更低的重心，在执行大负载
% 操作或高动态全身运动时，具备天然的静态与动态稳定性优势。
近年来，随着高功率密度电机、高能量密度电池以及轻量化材料技术的突破，足式机器人的
负载能力得到了显著提升，使其搭载机械臂成为可能。针对不同的应用需求与控制策略，
国内外多家顶级实验室与商业公司研发了多种形态各异的四足机械臂平台。
本节将重点回顾几类具有代表性的实验平台，并分析其硬件特性及优劣势。

\subsection{国外足式机械臂硬件平台}
2015年2月，波士顿动力发布了第一代名为 Spot（Spot Classic）的四足机器人，
该平台首次展示了四足机器人在室内狭窄空间与室外复杂地形下的卓越运动能力，
标志着高爆发电驱动技术在足式机器人领域的成熟应用\cite{ackerman2015spot}。
为了进一步适应非结构化的人居环境并拓展移动操作能力，波士顿动力于2016年6月
推出了SpotMini\cite{bostondynamics2016mini}。作为当前商业化 Spot 的直接原型，SpotMini 是首
个在四足底盘上深度集成全电动机械臂的实验平台，确立了“移动操作”的技术范式。
在控制架构方面，Spot 系列展现了卓越的全身协调控制能力。尽管具体实现细节
未完全公开，但业界普遍认为其沿用了波士顿动力在 Atlas 人形机器人上验证
成熟的分层式控制架构\cite{kuindersma2016optimization}：底层基于高频阻抗控制
律实现关节级的柔顺性与力矩跟踪；上层核心则广泛应用模型预测控制（MPC）来处理
非线性倒立摆模型的稳定性问题\cite{kuindersma2016optimization,}。特别是在涉及机械臂大动量操作
（如抛掷重物或开启重型门）的任务中，系统能够构建全身动力学方程，将机械臂运动
产生的反作用力与倾覆力矩作为前馈项引入底盘控制器，通过主动调整步态、支撑多边形
及关节力矩来维持系统平衡。

2018年，波士顿动力展示了 SpotMini 利用机械臂自主开启重型弹簧门的实验记录
，SpotMini 通过躯干的主动俯仰配合机械臂的末端轨迹，成功克服了弹簧阻力\cite{bostondynamics2018opendoor}。
在执行过程中，面对外界施加的强烈干扰，控制器能够实时调整全身姿态，
展现了极强的鲁棒性。然而，由于 Spot 系统的底层关节力矩控制器处于闭源状态，
且硬件成本高昂，其核心控制算法对学术界呈现“黑盒”特性\cite{gupta2021embodied}。
这在一定程度上限制了科研人员在该平台上直接验证底层全身动力学或端到端
强化学习力矩控制策略的可能性。

苏黎世联邦理工学院（ETH Zurich）研发的 ANYmal 系列及其衍生的 ALMA
（Articulated Locomotion and Manipulation）系统，代表了当前足式机械臂全身控制
研究在学术界的最高水准。与商业化平台不同，该系统自研发之初便强调驱动器的控制架构的开放性，
旨在解决高动态移动操作中的强耦合动力学问题\cite{hutter2016anymal}。

在硬件演进方面，ALMA 系统经历了从集成商用组件到全栈自研的过程。早期的 ALMA 系统
（约 2018 年）基于 ANYmal B 四足底盘，集成了 Kinova Jaco 轻量级机械臂。
尽管该配置验证了基本的全身协调算法，但由于商用机械臂的低带宽位置控制模式，
难以实现高频的力交互操作\cite{bellicoso2019alma}。为了突破这一瓶颈，ETH 团队
随后研发了专用的 DynaArm 机械臂\cite{sleiman2021unified,ma2022combining,sleiman2023versatile}，其采用了与 ANYmal 腿部
关节一致的高扭矩密度驱动技术。这一硬件升级使得系统能够感知微小的外界接触力，
为后续的高动态任务提供了物理基础。

在控制算法层面，该平台见证了从基于模型的预测控制（MPC）向基于强化学习（RL）的范式转移。
早期的研究主要采用非线性模型预测控制（NMPC）架构，通过构建全身动力学约束方程，
实时优化底盘步态与机械臂轨迹，成功实现了开启重型弹簧门、协同搬运等任务\cite{bellicoso2019alma}，
之后，该控制框架引入规划模块，实现了身体与机械臂、腿足与机械臂的配合效果，在结构化场景
中可以完成多种操作任务\cite{sleiman2021unified,sleiman2023versatile}。
然而，面对非结构化环境中的接触不确定性，基于模型的规划往往面临建模误差的挑战。
近年来，随着深度强化学习的发展，该团队在 ANYmal 平台上展示了基于学习的全身协同策略。
最新的研究成果显示，通过在物理仿真中进行大规模端到端（End-to-End）训练，
机器人能够掌握极具挑战性的高动态全身运动操作技能，如模拟照顾残疾人的精细抓取以及
涉及快速挥臂的类体育运动任务\cite{scheidemann2024cybathlonleggedmobile,doi:10.1126/scirobotics.adu3922}。
这些算法不仅利用了身体的配合姿态来扩展机械臂作业空间，还通过本体感知实现了对外界
扰动的自适应鲁棒控制。尽管该平台在性能上具备显著优势，但其高昂的硬件成本与复杂
的维护需求，限制了其在工业场景中的大规模普及，目前仍主要作为验证前沿控制理论的
科研载体。

\subsection{国内机械臂硬件平台}

自2017年以来，随着国内机器人产业链的成熟与高功率密度电机技术的突破，中国在足式机器人
领域迅速发展，涌现出以宇树科技（Unitree Robotics）和云深处科技（Deep Robotics）
为代表的领军企业。这些国产平台凭借准直驱（Quasi-Direct Drive, QDD）技术的广泛应用、
高度开放的底层控制接口以及模块化的硬件架构，迅速被全球科研机构采用，极大地推动了基于
强化学习的全身控制算法从仿真走向真机（Sim-to-Real）的验证进程。

在众多平台中，宇树科技研发的系列四足机器人（如 Aliengo, B1, Go2）配合其自研的Z1机
械臂，构成了当前学术界最为通用的实验载体。特别是Z1机械臂，专为移动机器人设计，具有
极高的力矩自重比和与底盘一致的控制协议。研究人员利用该平台，能够将四足底盘与机械臂
视为统一的动力学系统，在仿真环境（如 Isaac Lab）中进行大规模并行训练，进而将端到
端的神经网络策略直接部署于物理样机\cite{fu2022coupling, ji2022concurrent}。
这种全栈式的硬件生态解决了以往异构系统中通信延迟高、控制带宽不匹配的难题。

另一方面，云深处科技推出的“绝影”（Jueying）系列机器人（如 X20, X30）则深耕工业
巡检与特种作业场景。该类平台强调复杂地形下的运动稳定性与高负载能力，常被用于验证基于
模型预测控制（MPC）的重载移动操作任务。尽管其部分底层接口主要面向行业应用优化，
但其稳定的状态估计与运动原语为上层长周期规划算法提供了可靠的验证环境\cite{wang2021model}。

此外，随着轮足复合技术的兴起，国内亦出现了如逐际动力（LimX Dynamics）W1等新型移动
操作平台，进一步丰富了足式机械臂的研究形态。
综上所述，国产高性能平台的普及降低了全身控制研究的硬件门槛，
但轻量化平台中显著的机臂惯性耦合效应，也对控制算法的抗扰动能力提出了更高的要求。


\section{足式机械臂控制方法概述}

\subsection{模型预测控制}

\subsubsection{分层控制框架}
在动力学建模层面，为了平衡计算效率与预测精度，MPC 规划层通常采用扩展的单刚体动力学模型。与传统仅将底盘视为刚体的方法不同，该模型将底盘与机械臂视为一个具有时变惯量特性的复合刚体系统。

设系统的状态向量为 $\mathbf{x} \in \mathbb{R}^n$，控制输入向量为 $\mathbf{u} \in \mathbb{R}^m$。状态向量 $\mathbf{x}$ 包含全身复合质心位置 $\mathbf{p}_{\text{com}}$、质心速度 $\mathbf{v}_{\text{com}}$、基座姿态欧拉角 $\boldsymbol{\Theta}_b$、基座角速度 $\boldsymbol{\omega}_b$ 以及机械臂的关节角度 $\mathbf{q}_{\text{arm}}$ 与关节速度 $\dot{\mathbf{q}}_{\text{arm}}$。系统的动力学演化遵循牛顿-欧拉方程，其具体形式如下：
\begin{subequations}
      \begin{align}
            \dot{\mathbf{p}}_{\text{com}}                                                                      & = \mathbf{v}_{\text{com}}                                                                                                               \\
            m_{\text{sys}} \dot{\mathbf{v}}_{\text{com}}                                                       & = \sum_{i=1}^{n_c} \mathbf{f}_i + m_{\text{sys}} \mathbf{g} \label{eq:linear_dyn}                                                       \\
            \frac{d}{dt} \left( \mathbf{I}_{\text{sys}}(\mathbf{q}_{\text{arm}}) \boldsymbol{\omega}_b \right) & = \sum_{i=1}^{n_c} (\mathbf{r}_i - \mathbf{p}_{\text{com}}) \times \mathbf{f}_i + \boldsymbol{\tau}_{\text{ext}} \label{eq:angular_dyn}
      \end{align}
\end{subequations}
其中，式(\ref{eq:linear_dyn}) 描述了线性动量变化率，式(\ref{eq:angular_dyn}) 描述了角动量变化率。

为了构建适用于模型预测控制的标准状态空间模型，我们需要将上述物理方程重写为一阶微分形式。通过结合机械臂的运动学关系 $\dot{\mathbf{q}}_{\text{arm}} = \mathbf{v}_{\text{arm}}$，并将式(\ref{eq:linear_dyn})与式(\ref{eq:angular_dyn})整理为关于状态导数 $\dot{\mathbf{v}}_{\text{com}}$ 与 $\dot{\boldsymbol{\omega}}_b$ 的显式表达式，可得到系统的非线性状态空间方程：
\begin{equation}
      \dot{\mathbf{x}}(t) =
      \begin{bmatrix}
            \dot{\mathbf{p}}_{\text{com}} & \dot{\mathbf{v}}_{\text{com}} & \dot{\boldsymbol{\Theta}}_b & \dot{\boldsymbol{\omega}}_b & \dot{\mathbf{q}}_{\text{arm}} & \ddot{\mathbf{q}}_{\text{arm}}
      \end{bmatrix}^T
      = f(\mathbf{x}(t), \mathbf{u}(t))
      \label{eq:state_space_func}
\end{equation}
该函数 $f(\cdot)$ 完整映射了系统状态随控制输入的演化规律。需要特别指出的是，方程中的 $\mathbf{I}_{\text{sys}}(\mathbf{q}_{\text{arm}}) \in \mathbb{R}^{3 \times 3}$ 表示系统的复合刚体惯性张量。与传统四足机器人恒定的躯干惯量不同，足式机械臂的惯性分布会随着机械臂关节角度 $\mathbf{q}_{\text{arm}}$ 的变化而显著改变。因此，在推导 $\dot{\boldsymbol{\omega}}_b$ 时（即构建函数 $f$ 的角速度分量时），必须将角动量的变化率展开为包含陀螺力矩项的形式：
\begin{equation}
      \mathbf{I}_{\text{sys}}(\mathbf{q}_{\text{arm}}) \dot{\boldsymbol{\omega}}_b + \underbrace{\left( \boldsymbol{\omega}_b \times \mathbf{I}_{\text{sys}}(\mathbf{q}_{\text{arm}}) \boldsymbol{\omega}_b + \dot{\mathbf{I}}_{\text{sys}}(\mathbf{q}_{\text{arm}}, \dot{\mathbf{q}}_{\text{arm}}) \boldsymbol{\omega}_b \right)}_{\text{非线性耦合项}} = \boldsymbol{\tau}_{\text{total}}
\end{equation}
上式清晰地表明，机械臂的快速运动（体现为 $\dot{\mathbf{q}}_{\text{arm}}$ 导致的 $\dot{\mathbf{I}}_{\text{sys}}$ 项）会直接在基座上产生附加的惯性力矩耦合。如果忽略这一时变特性，将 $\mathbf{I}_{\text{sys}}$ 视为常数，则会在 MPC 预测视界内引入显著的模型误差。

基于上述建立的状态空间模型 $f(\mathbf{x}, \mathbf{u})$，模型预测控制的核心在于在每一个控制周期内，求解一个有限时间视界 $T$ 内的开环最优控制问题。该优化问题旨在寻找最优控制序列 $\mathbf{u}^*(\cdot)$，使得系统的状态轨迹在满足物理约束的前提下尽可能跟踪参考轨迹。其数学表达如下所示：
\begin{subequations}
      \begin{align}
            \min_{\mathbf{u}(\cdot)} \quad & \int_{0}^{T} \left( \|\mathbf{x}(t) - \mathbf{x}_{\text{ref}}(t)\|_{\mathbf{Q}}^2 + \|\mathbf{u}(t)\|_{\mathbf{R}}^2 \right) dt + \|\mathbf{x}(T) - \mathbf{x}_{\text{ref}}(T)\|_{\mathbf{P}}^2 \\
            \text{s.t.} \quad              & \dot{\mathbf{x}}(t) = f(\mathbf{x}(t), \mathbf{u}(t)) \label{eq:dynamics_constraint}                                                                                                            \\
                                           & \mathbf{f}_i(t) \in \mathbb{C}(\mu, \mathbf{n}) \quad \forall i \in \{1, \dots, n_c\} \label{eq:friction}                                                                                       \\
                                           & \mathbf{p}_{\text{foot}, i}(t) \in \mathbb{S}_{\text{terrain}} \label{eq:terrain}                                                                                                               \\
                                           & \mathbf{x}_{\text{min}} \leq \mathbf{x}(t) \leq \mathbf{x}_{\text{max}}, \quad \mathbf{u}_{\text{min}} \leq \mathbf{u}(t) \leq \mathbf{u}_{\text{max}} \label{eq:limits}
      \end{align}
\end{subequations}
上述公式中，目标函数包含过程代价与终端代价，矩阵 $\mathbf{Q}, \mathbf{R}, \mathbf{P}$ 分别为状态误差、控制输入及终端误差的权重矩阵。约束条件(\ref{eq:dynamics_constraint})即为前文推导的包含惯性耦合效应的系统动力学约束；式(\ref{eq:friction})为接触力摩擦锥约束，确保足端不发生打滑；式(\ref{eq:terrain})为地形几何约束，限制落足点在可行区域内；式(\ref{eq:limits})为关节位置、速度及力矩的物理限位约束。

由于 MPC 规划层的更新频率通常较低，且基于简化模型，为了实现高频且精准的运动执行，控制系统下层通常配备一个基于全阶动力学的全身控制器（WBC）。WBC 接收 MPC 输出的期望反作用力 $\mathbf{f}_{\text{mpc}}$ 与运动轨迹 $\mathbf{x}_{\text{mpc}}$，通过求解二次规划（Quadratic Programming, QP）问题，将任务空间的加速度指令映射为各关节的力矩指令 $\boldsymbol{\tau}$。该层级能够利用全身冗余自由度处理任务优先级，例如在保证质心平衡的首要任务下，尽可能减小机械臂末端的操作误差。这种分层递阶的控制架构有效地解决了足式机械臂长时间跨度规划与瞬时高频响应之间的矛盾，构成了基于模型控制方法的核心逻辑\cite{bledt2018policy, bellicoso2017dynamic}。

\subsubsection{基于MPC分层控制框架的足式机械臂全身控制}
基于上述分层控制理论，早期的足式机械臂研究往往采用解耦的控制策略，即将机械臂视为对四足底盘的外部扰动，或仅在运动学层面进行协调。然而，当机械臂执行快速挥动或重载操作时，忽略两者之间的动力学耦合会导致底盘稳定性显著下降。针对这一问题，苏黎世联邦理工学院的研究团队在ALMA\cite{bellicoso2019alma}平台上提出了一系列改进方案，实现了从解耦控制向统一全身控制的跨越。

2021年，Sleiman 等人提出了一种统一全身 MPC 框架\cite{sleiman2021unified}，其核心创新在于扩展了系统的状态空间与动力学模型。与传统方法仅将身体视为单刚体不同，该框架将机械臂的关节位置 $\mathbf{q}_{arm}$ 与速度 $\dot{\mathbf{q}}_{arm}$ 显式纳入 MPC 的预测状态向量 $\mathbf{x}$ 中。此时，系统的全身惯性张量 $\mathbf{I}(\mathbf{q}_{arm})$ 不再被视为常数，而是被建模为随机械臂构型变化的函数。在优化目标函数中，引入了针对机械臂末端位置 $\mathbf{p}_{ee}$ 的跟踪项：
\begin{equation}
      J_{track} = \int_{0}^{T} \left( \|\mathbf{p}_{com} - \mathbf{p}_{com}^{ref}\|_{\mathbf{Q}_{base}}^2 + \|\mathbf{p}_{ee}(\mathbf{q}_{arm}) - \mathbf{p}_{ee}^{ref}\|_{\mathbf{Q}_{arm}}^2 \right) dt
\end{equation}
通过联合优化四足机器人与机械臂的运动，该控制器能够产生涌现式的协同行为。例如，当机械臂试图抓取超出当前作业空间的物体时，MPC 会自动规划出身体前倾或调整步态的策略，利用全身冗余度来扩展操作范围，而无需人为指定底盘的配合动作。

在此基础上，Mittal 等人进一步提出了多接触规划与控制框架\cite{sleiman2023versatile}，旨在解决涉及复杂环境交互的“移动操作”问题。该方法将足式机械臂系统建模为混合动力学系统，不仅考虑足端的地面反作用力，还将机械臂末端与环境的接触力 $\mathbf{f}_{ee}$ 纳入质心动力学方程：
\begin{equation}
      \dot{\mathbf{h}}_{com} = \sum_{i=1}^{n_{foot}} \mathbf{f}_{foot, i} + \mathbf{f}_{ee} + m\mathbf{g}
\end{equation}
该框架引入了开关系统理论来处理接触模式的离散切换，允许求解器在“仅足端支撑”与“足-手同时支撑”等模式间进行决策。在 ANYmal 机器人的硬件实验中，该方法成功实现了利用全身惯量开启重型弹簧门、在非平整地形上推拉重物以及利用墙壁支撑保持平衡等高动态任务，验证了基于模型的统一规划方法在处理复杂力交互方面的有效性。

尽管基于 MPC 的全身控制方法在理论完备性与可解释性上表现优异，但在实际应用中仍面临显著的局限性，这也推动了学术界向强化学习方向的探索：
\begin{itemize}
      \item \textbf{对模型精度的强依赖}：MPC 的性能高度依赖于动力学模型 $f(\mathbf{x}, \mathbf{u})$ 的准确性。在涉及抓取未知质量物体或在松软地面行走的场景中，惯性参数与摩擦系数的估计误差会导致预测轨迹与实际严重偏离，甚至引发系统发散。
      \item \textbf{非线性优化的实时性瓶颈}：为了处理全动力学约束，非线性 MPC 通常需要极其昂贵的计算资源。为了保证控制频率，现有的实现往往不得不缩短预测视界或简化模型，限制了机器人执行大跨度、高爆发动作的能力。
      \item \textbf{接触序列的预定义限制}：目前的 MPC 框架大多需要预先人为指定接触顺序。在极端复杂的非结构化环境中，依靠优化算法自主发现全新的接触策略（如意外摔倒时的支撑保护）仍然极其困难。
\end{itemize}
正是由于上述局限性，无需显式建模且具备在大规模数据中自主涌现复杂策略能力的强化学习方法，逐渐成为解决足式机械臂全身控制问题的新研究热点。

\subsection{强化学习}

强化学习作为机器学习的重要分支之一，其理论框架受到动物行为心理学与现代最优控制理论的影响。从爱德华·桑代克提出的“效果律”——即带来满意结果行为应被奖励，到理查德·贝尔曼在控制理论中奠定的动态规划基础，强化学习逐渐发展为一种通用的决策优化方法。它主要研究智能体如何在未知或部分未知的环境中，通过序列决策与试错机制，学习出能够最大化长期累积奖励的最优策略。

\subsubsection{强化学习问题建模与贝尔曼方程}

在足式机械臂的全身控制任务中，机器人的决策过程可以被建模为一个无限视界的马尔可夫决策过程（Markov Decision Process, MDP）。MDP 提供了一个标准化的框架来描述处于随机动态环境中的交互过程。

一个标准的马尔可夫决策过程由五元组 $\mathbb{M} = \langle \mathbb{S}, \mathbb{A}, \mathbb{P}, \mathbb{R}, \gamma \rangle$ 定义，其中各个元素物理意义如下：

\textbf{1. 状态空间 $\mathbb{S}$}
状态空间 $\mathbb{S}$ 定义了环境所有可能状态的集合。针对足式机械臂这一高维非线性系统，$\mathbb{S} \subseteq \mathbb{R}^n$ 是一个连续的欧几里得空间。时刻 $t$ 的状态向量 $s_t \in \mathbb{S}$ 通常包含机器人的本体感知信息，具体包括：基座的线性速度与角速度、重力向量在基座坐标系下的投影、各关节的角度位置以及角速度等。状态 $s_t$ 必须具备马尔可夫性，即包含预测未来所需的所有历史信息。

\textbf{2. 动作空间 $\mathbb{A}$}
动作空间 $\mathbb{A}$ 定义了智能体可执行的所有行为的集合。在连续控制任务中，$\mathbb{A} \subseteq \mathbb{R}^m$。时刻 $t$ 的动作向量 $a_t \in \mathbb{A}$ 直接对应于机器人的控制指令，常见的形式包括关节的目标位置、速度或力矩。

\textbf{3. 状态转移概率 $\mathbb{P}$}
状态转移概率 $\mathbb{P}: \mathbb{S} \times \mathbb{A} \times \mathbb{S} \rightarrow [0, \infty)$ 描述了环境的随机动力学模型。它表示在当前时刻 $t$ 处于状态 $s_t$ 并执行动作 $a_t$ 后，下一时刻 $t+1$ 系统状态服从的概率密度函数：
\begin{equation}
      s_{t+1} \sim \mathbb{P}(\cdot | s_t, a_t)
\end{equation}
对于足式机器人，该转移过程隐含了刚体动力学方程、地面接触模型以及摩擦约束，通常是高度非线性且难以解析建模的。

\textbf{4. 奖励函数 $\mathbb{R}$}
奖励函数 $\mathbb{R}: \mathbb{S} \times \mathbb{A} \rightarrow \mathbb{R}$ 是强化学习的核心引导信号。在每一时间步 $t$，智能体根据当前状态和动作获得一个标量即时奖励 $r_t = \mathbb{R}(s_t, a_t)$。该函数将复杂的控制目标（如保持平衡、跟踪速度、最小化能耗）量化为数值信号，直接决定了策略的学习方向。通常在强化学习中使用回报 $G_t$ 来评估策略在长期运行中的表现。$G_t$ 被定义为从时刻 $t$ 开始，未来所有时间步的折扣奖励之和：
\begin{equation}
      G_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \dots = \sum_{k=0}^{\infty} \gamma^k r_{t+k}
\end{equation}

\textbf{5. 折扣因子 $\gamma$}
折扣因子 $\gamma \in [0, 1)$ 用于权衡即时奖励与长期奖励的相对重要性。在数学上，$\gamma < 1$ 保证了在无限视界下，累积回报级数是收敛的，避免了数值发散的问题。

基于上述MDP模型，强化学习旨在寻找一个最优策略$\pi^*$，使得在该策略下得到的累积期望回报最大化：
\begin{equation}
      \underset{\pi}{\arg \max} \; J(\pi) = \mathbb{E}_{\tau \sim \pi} [G_0]
\end{equation}
其中，期望 $\mathbb{E}$ 是对轨迹 $\tau = (s_0, a_0, r_0, s_1, \dots)$ 的分布求得的。

在深度强化学习中，随机策略 $\pi$使用$\theta$进行参数化表示，通常指代深度神经网络的权重参数。策略$\pi_{\theta}$定义了在状态 $s_t$ 下选择动作 $a_t$ 的条件概率分布：
\begin{equation}
      a_t \sim \pi_\theta(\cdot | s_t)
\end{equation}

由于未来的状态转移不可微分且具有随机性，直接优化回报 $G_t$ 较为困难。因此，在强化学习中，引入价值函数来估计策略的期望性能。常用的价值函数有两种：一是状态价值函数 $V^\pi(s)$：表示在状态 $s$ 下，遵循策略 $\pi_{\theta}$ 直至终止所能获得的期望回报：
\begin{equation}
      V^\pi(s) = \mathbb{E}_{\pi_\theta} [G_t | s_t = s]
\end{equation}
二是动作价值函数$Q^\pi(s, a)$：表示在状态 $s$ 下执行动作 $a$，随后遵循策略 $\pi_{\theta}$ 所能获得的期望回报：
\begin{equation}
      Q^\pi(s, a) = \mathbb{E}_{\pi_\theta} [G_t | s_t = s, a_t = a]
\end{equation}

此时，利用回报定义的递归性质 $G_t = r_t + \gamma G_{t+1}$，可以将价值函数展开为当前奖励与下一时刻价值的组合。这被称为贝尔曼期望方程，它是评估策略优劣的理论基础：
\begin{equation}
      \begin{aligned}
            V^\pi(s)    & = \mathbb{E}_{a \sim \pi(\cdot|s)} \left[ \mathbb{E}_{s' \sim \mathbb{P}(\cdot|s, a)} \left[ r(s,a) + \gamma V^\pi(s') \right] \right] \\
            Q^\pi(s, a) & = \mathbb{E}_{s' \sim \mathbb{P}(\cdot|s, a)} \left[ r(s,a) + \gamma \mathbb{E}_{a' \sim \pi(\cdot|s')} [Q^\pi(s', a')] \right]
      \end{aligned}
      \label{eq:bellman_expectation}
\end{equation}
该方程说明，当前状态的价值等于即时奖励的期望加上经折扣后的下一状态价值的期望。

强化学习的目的是控制，即寻找最优策略 $\pi^*$，使得对于任意状态 $s$，都有 $V^{\pi^*}(s) \geq V^\pi(s)$。最优策略对应的价值函数称为最优状态价值函数 $V^*(s)$ 和最优动作价值函数 $Q^*(s, a)$。它们满足贝尔曼最优方程：
\begin{equation}
      \begin{aligned}
            V^*(s)    & = \max_{a \in \mathbb{A}} \mathbb{E}_{s' \sim \mathbb{P}(\cdot|s, a)} \left[ r(s,a) + \gamma V^*(s') \right]      \\
            Q^*(s, a) & = \mathbb{E}_{s' \sim \mathbb{P}(\cdot|s, a)} \left[ r(s,a) + \gamma \max_{a' \in \mathbb{A}} Q^*(s', a') \right]
      \end{aligned}
      \label{eq:bellman_optimality}
\end{equation}
与期望方程不同，最优方程中引入了最大化算子（$\max$），这不仅体现了策略优化的目标，也引入了非线性特性。该方程是 Q-Learning\cite{} 以及 Actor-Critic\cite{} 架构中Critic更新的基础 。

\subsubsection{强化学习主要架构}

在深度强化学习的发展历程中，强化学习算法架构经历了从基于价值（Value-based）到基于策略（Policy-based）的演进。目前，足式机械臂控制领域最为主流的架构是结合了两者优势的 Actor-Critic 架构。此外，为了解决部分可观测环境下的控制难题并提升 Sim-to-Real 的鲁棒性，非对称 Actor-Critic 架构应运而生，并成为本论文所采用的核心框架。

标准的 Actor-Critic (AC) 架构由两个独立的神经网络组成：Actor（策略网络）和 Critic（价值网络）。这种架构旨在解决纯策略梯度方法方差过大以及纯价值方法难以处理连续动作空间的问题。
其中，Actor网络（$\pi_{\theta}$）输入是当前状态 $s_t$，输出是动作的概率分布。
Critic 网络 ($V_\phi$ 或 $Q_\phi$): 输入是当前状态 $s_t$（或$s_t$， $a_t$），输出是对当前策略好坏的评估值。其作用是辅助 Actor 更新，起到基线的作用以降低梯度估计的方差。

Actor 的优化目标是最大化期望回报 $J(\theta)$。根据策略梯度定理（Policy Gradient Theorem），目标函数的梯度可以表示为：
\begin{equation}
      \nabla_\theta J(\theta) = \mathbb{E}_{s_t, a_t \sim \pi_\theta} \left[ \nabla_\theta \log \pi_\theta(a_t | s_t) A^{\pi_\theta}(s_t, a_t) \right]
\end{equation}
其中，$A^{\pi_\theta}(s_t, a_t)$ 被称为优势函数（Advantage Function），它量化了在状态 $s_t$ 下执行动作 $a_t$ 相对于平均表现的优势程度。在 Actor-Critic 架构中，优势函数通常通过 Critic 网络的输出来估计。最常用的估计形式基于时间差分误差（TD-Error）：
\begin{equation}
      A(s_t, a_t) \approx \delta_t = r_t + \gamma V_\phi(s_{t+1}) - V_\phi(s_t)
\end{equation}
这里，$\delta_t$ 直观地表示了“真实发生的奖励加上对未来的预期”与“当前预期”之间的差值。

相应地，Critic 的任务是尽可能准确地逼近真实的状态价值 $V^{\pi}(s)$。其训练通常被建模为一个回归问题，即最小化预测价值与目标价值之间的均方误差：
\begin{equation}
      L(\phi) = \mathbb{E}_{s_t, r_t, s_{t+1}} \left[ \left( (r_t + \gamma V_\phi(s_{t+1})) - V_\phi(s_t) \right)^2 \right]
\end{equation}
通过交替更新 $\theta$ 和 $\phi$，Actor 指导 Critic 探索高回报区域，而 Critic 指导 Actor 使得动作概率向高价值方向移动。

在足式机械臂的实际控制任务中，环境往往是部分可观测的。例如，机器人自身的传感器只能测量本体姿态和关节状态，而无法直接获取地形的摩擦系数、地面的精确高度图或外部扰动力。然而，在物理仿真环境中，这些信息是完全可知的。

为了充分利用仿真环境中的特权信息（Privileged Information）来加速训练，同时保证策略网络能够仅依赖传感器数据在真机上部署，本论文采用了非对称 Actor-Critic 架构。

该架构的核心在于 Actor 和 Critic 网络在训练阶段接收不同的输入空间：
\begin{itemize}
      \item \textbf{Actor 输入（观测信息 $o_t$）}: 包含机器人实际部署时可获取的传感器数据（如关节位置、IMU 数据）。记观测空间为 $\mathbb{O}$。
      \item \textbf{Critic 输入（特权信息 $s_t^{priv}$）}: 不仅包含不含噪声的 $o_t$，还包含仿真引擎提供的特权信息，如地形物理属性、接触力、机器人质心速度真值等。记特权状态空间为 $\mathbb{S}_{priv}$。
\end{itemize}

在这种架构下，Actor 网络 $\pi_\theta(a_t | o_t)$ 旨在学习一个仅依赖局部观测的策略，而 Critic 网络 $V_\phi(s_t^{priv})$ 则利用全局状态来提供更准确的价值估计。
\begin{equation}
      \begin{aligned}
            a_t & \sim \pi_\theta(\cdot | o_t) \\
            V_t & \approx V_\phi(s_t^{priv})
      \end{aligned}
\end{equation}

在计算优势函数时，我们利用拥有特权信息的 Critic 来指导仅拥有观测信息的 Actor：
\begin{equation}
      \nabla_\theta J(\theta) \approx \mathbb{E} \left[ \nabla_\theta \log \pi_\theta(a_t | o_t) \left( r_t + \gamma V_\phi(s_{t+1}^{priv}) - V_\phi(s_t^{priv}) \right) \right]
\end{equation}

非对称架构在工程实践中具有双重意义：
\begin{enumerate}
      \item \textbf{增强训练稳定性}: Critic 能够观测到如“地面摩擦力突变”或“外力干扰”等隐藏变量，因此能给出更平滑、低方差的价值评估，避免了因为状态混淆导致的训练震荡。
      \item \textbf{零样本迁移}: 在真机部署阶段，Critic 网络被抛弃，仅保留 Actor 网络。由于 Actor 在训练时就被强制要求仅利用 $o_t$ 做出决策，因此它可以直接移植到物理机器人上，无需依赖现实中无法观测的特权信息。
\end{enumerate}

这种非对称架构已成为近年来足式机器人强化学习领域的标准范式。

\subsubsection{强化学习主流算法}
在确立了 Actor-Critic 架构后，如何有效地更新网络参数以保证策略的持续改进成为了核心问题。根据数据利用方式的不同，现代深度强化学习算法主要分为两大类：以 PPO (Proximal Policy Optimization) 为代表的同策略（On-Policy）算法，和以 SAC (Soft Actor-Critic) 为代表的异策略（Off-Policy）算法。这两种算法因其在连续控制任务中的卓越表现，成为了足式机械臂控制领域的基石。

\paragraph{近端策略优化算法 (PPO)}

在连续高维的机器人控制任务中，策略更新的步长至关重要。若步长过小，训练收敛缓慢；若步长过大，策略可能发生剧烈震荡甚至导致性能下降。为了解决这一问题，Schulman 等人提出了 PPO 算法，其核心思想是限制每次更新时新旧策略之间的差异，确保优化过程在“信任区域”内进行。

PPO 通过引入新旧策略的概率比率 $r_t(\theta)$ 来量化更新幅度：
\begin{equation}
      r_t(\theta) = \frac{\pi_\theta(a_t | s_t)}{\pi_{\theta_{old}}(a_t | s_t)}
\end{equation}
在此基础上，PPO 构造了一个被截断的目标函数，用于替代传统的策略梯度目标。该目标函数取原始目标与截断目标的最小值，从而形成一个下界估计：
\begin{equation}
      L^{CLIP}(\theta) = \mathbb{E}_t \left[ \min \left( r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t \right) \right]
\end{equation}
其中，$\hat{A}_t$ 为优势函数的估计值，$\epsilon$ 为超参数（通常取 0.2），用于限制 $r_t(\theta)$ 在 $[1-\epsilon, 1+\epsilon]$ 区间内。$\text{clip}$ 操作通过强制截断策略更新的幅度，消除了因样本噪声引起的过大梯度更新风险。

凭借其在训练稳定性、调参鲁棒性以及实现复杂度上的优势，PPO 已成为目前足式机器人运动控制的首选算法。它能够保证机器人在学习复杂步态时，策略性能表现呈单调提升趋势，极大地降低了训练发散的概率。

\paragraph{Soft Actor-Critic (SAC)}

与 PPO 必须使用当前策略采集样本不同，SAC 属于异策略算法，能够利用历史经验池中的数据进行训练，因此具有更高的样本效率。SAC 的理论基础是最大熵强化学习（Maximum Entropy RL），其目标不仅仅是最大化累积回报，还要最大化策略的熵，即鼓励动作的多样性和随机性。

在最大熵框架下，最优策略的优化目标修改为：
\begin{equation}
      J(\pi) = \sum_{t=0}^{T} \mathbb{E}_{(s_t, a_t) \sim \rho_\pi} \left[ r(s_t, a_t) + \alpha \mathbb{H}(\pi(\cdot | s_t)) \right]
\end{equation}
其中，$\mathbb{H}(\cdot)$ 表示策略分布的信息熵，$\alpha$ 为温度系数，用于调节奖励与熵之间的权重。熵项的引入具有重要的物理意义：它鼓励机器人在训练初期广泛探索状态空间，防止策略过早陷入局部最优，同时也增强了策略面对环境扰动时的鲁棒性。

尽管 SAC 在样本效率上优于 PPO，但其对超参数较为敏感，且在极高维度的全身协调控制任务中，其训练稳定性往往略逊于 PPO。因此，在现有的足式机械臂研究中，PPO 通常用于处理对稳定性要求极高的全身动力学控制，而 SAC 则常被用于数据获取成本较高的真实机器人在线微调任务。

\subsubsection{足式机械臂强化学习全身控制现状}

足式机械臂系统的全身控制旨在解决浮动基座与多自由度机械臂之间的非线性动
力学耦合问题。随着深度强化学习的兴起，该领域的研究呈现出从分层解耦向
端到端统一控制演进的趋势。

为了缓解高维状态空间带来的探索与优化困难，早期的主流研究倾向于采用解耦控制策略，即将复杂的全身控制问题拆解为底盘移动与机械臂操作两个相对独立的子任务。在这一技术路线中，Yuntao等人\cite{ma2022combining}提出了一种具有代表性的混合控制架构：四足机器人部分采用强化学习训练出的鲁棒策略进行控制，以适应复杂地形；而机械臂部分则交由基于模型的运动学规划器进行求解。尽管这种的处理方式缓解了全身控制难度，确保了机器人在移动过程中的基础稳定性，但这种方法本质上将机械臂的运动视为对身体的外部扰动。因此，由于缺乏子系统间的统一协调机制，该架构难以应对涉及剧烈动态交互的复杂全身任务。

针对跨场景泛化难题，Jilong等人 \cite{wang2025quadwbggeneralizablequadrupedalwholebody} 提出了模块化框架 QuadWBG。与端到端控制不同，该工作侧重于足式机械臂执行抓取任务的可达性规划，通过构建广义定向可达性图（GORM）来优化基座的停靠位置 $\mathbf{p}_{base}^*$。虽然该方法通过显式的几何推理实现了足式机械臂对未见物体的抓取的泛化性，但其本质是先移动到位，再执行抓取的序列化流程。这种在时间域上的控制不连续性，使得系统无法在移动过程中动态调整全身姿态以应对突发扰动，难以满足高动态全身控制的实时性要求。

鉴于解耦方法的各种问题，研究界开始探索利用深度强化学习直接训练端到端的全身控制器。然而，在一个高维、非凸且存在严重物理约束的动作空间中同时优化移动与操作策略并非易事。四足机器人了的移动任务通常收敛较快且梯度较大，而机械臂的精确操作则需要精细的微调和身体的姿态配合，这种优化目标的不均衡往往导致策略网络陷入局部最优，甚至出现为了移动任务而放弃操作的退化行为。

为了克服这一训练难题，Zipeng等人\cite{fu2023deep}率先提出了一种基于优势混合（Advantage Mixing）的训练范式。优势混合是指在策略优化的过程中，先将机械臂目标任务与四足机器人移动目标任务对应的优势函数进行分离，让策略先学习独立的任务。随后，在训练过程中，通过课程参数$\beta$将操作与移动优势函数进行混合，让策略学习耦合任务的期望动作。这种方法通过降低了信用分配的复杂程度，即先将操作任务归因于机械臂动作，将移动任务归因于四足机器人的动作，然后逐步退火从而降低了学习难度，实现了端到端的足式机械臂的全身控制。
\begin{equation}
      J(\theta) = \frac{1}{|D|} \sum_{(s, a) \in D} \log \pi(a^{arm}|s) (A^{mani} + \beta A^{loco}) + \log \pi(a^{quad}|s) (A^{mani} + \beta A^{lco})
\end{equation}
在上式中，$\beta$是课程参数，在训练过程中从0逐步变为1。$A^{mani}$与$A^{loco}$是操作任务与运动任务对应的优势函数。$a^{arm}$与$a^{quad}$分别对应机械臂动作与四足机器人动作。

此外，为了解决 Sim-to-Real 的问题，该工作引入了正则化在线适应（Regularized Online Adaptation）机制。即在训练过程中，策略网络的编码器输出不断的向带有特权信息的编码器的输出特征逼近，最终实现了策略网络追踪性能的提升。最终得到的控制器展现出了令人印象深刻的涌现行为。例如，机器人在够取远处物体时，会自动调整身体姿态以配合机械臂，这种全身协同是传统分层方法难以通过人工设计规则实现的。

不同于从梯度层面入手，Guoping等人提出的 RoboDuet \cite{pan2024roboduet} 选择了从训练流程上降低任务耦合度。该框架采用两阶段课程学习：第一阶段专注于训练一个鲁棒的四足移动策略；第二阶段则引入机械臂任务，利用机械臂的位姿追踪奖励对整个网络进行全身微调。在第二阶段中，原有的运动策略并未被冻结，而是与机械臂策略同步更新。这种分阶段的方法有效规避了从零开始训练的探索陷阱，成功获得了具备高度协调性的全身控制器。尽管上述方法取得了不错的控制效果，但其成功高度依赖于复杂的训练技巧。比如优势混合方法引入了课程参数的动态退火，而两阶段训练则延长了优化周期。这些方法本质上是在权衡移动与操作任务，增加了调参的工程难度。如果缺乏精细设计的课程机制，策略网络极易陷入局部最优。

针对具有非完整约束的特殊形态，Jiang等人 \cite{Jiang_2025} 将研究对象扩展至轮足机械臂。面对轮式滚动与腿部踏步的复杂混合动力学，该工作提出了奖励融合机制，精心设计了一套包含基座速度跟踪、末端执行器位姿惩罚以及关节平滑约束的复合奖励函数设计方法，通过端到端的联合优化，该控制器能够自适应地协调轮毂电机的速度与髋膝关节的力矩，在保证全向移动灵活性的同时实现了高精度的操作。

随着基础架构的成熟，研究重心开始向更复杂的任务场景转移。针对接触力控任务，相关工作 \cite{portela2024learning,zhi2025learningunifiedpolicyposition} 通过设计位置-力混合奖励函数，实现了在足式机械臂平台下的擦拭、按压与拉拽等移动操作任务。Ha等人的 UMI on Legs \cite{ha2024umilegsmakingmanipulation} 则采用了“以操作为中心”的数据驱动思路，利用手持式通用操作接口（UMI）采集的人类演示数据引导策略训练，极大地提升了操作的灵巧度。

在高动态技能方面，Yuntao等人\cite{doi:10.1126/scirobotics.adu3922} 展示了机器人如何通过预测球体轨迹并协调全身关节，完成打羽毛球这一极具挑战性的任务；同时Yuntao等人 \cite{ma2023learningarmassistedfalldamage} 还探索了防御性策略，利用机械臂辅助支撑以减少跌倒损伤。尽管这些工作拓展了 RL 的应用边界，但它们往往依赖于特定的任务约束或昂贵的外部感知设备，且对模型参数高度敏感，限制了控制器在部分环境下的鲁棒性。

\section{本章小结}

综上所述，现有的分层架构限制了潜能释放，而复杂的端到端训练范式增加了工程门槛，并且训练过程策略十分容易陷入局部最优。鉴于此，本论文旨在探索一种基于PPO强化学习算法与非对称 Actor-Critic 架构的端到端训练方法。不同于复杂的优势混合或显式适应模块，本研究将通过非对称的信息输入——即在训练阶段 Critic 网络利用包含环境物理参数的全知信息，而 Actor 网络仅依赖本体感知——来加速收敛速度，通过引入物理可行域奖励设计方法天然的建立操作任务与移动任务的联系。这种架构旨在在不引入复杂课程与额外估计器的情况下。实现高效、鲁棒且具备强泛化能力的全身控制。接下来的几章将详细阐述全身控制训练方法与部署的具体实现。


\chapter{基于强化学习的足式机械臂全身运动控制}


\chapter{物理可行域引导的奖励设计方法}

\chapter{待定}


% \chapter{关于本模板}

% 本模板根据浙江大学研究生院编写的《浙江大学研究生学位论文编写规则》~\cite{zjugradthesisrules}，
% 在原有的 zjuthesis 模板~\cite{zjuthesis}基础上开发而来。

% 本模板的本科生版本\cite{zjuthesisrules}得到了浙江大学本科生院老师的支持与审核，
% 已经在本科生院网上公示。
% 但当前的研究生版本并未经过研究生院老师的审核，
% 同学们使用时要注意对照模板与要求，
% 切不可盲目使用。

% 作者本人并未编写过浙江大学研究生毕业论文，
% 所以不清楚具体要求。
% 如果有热心同学愿意帮忙，
% 可以替我联系相关老师，我会配合审核并修改代码。

% \section{Overleaf 使用注意事项}

% 如果你在Overleaf上编译本模板，请注意如下事项：

% \begin{itemize}
%     \item 删除根目录的 ``.latexmkrc'' 文件，否则编译失败且不报任何错误
%     \item 字体有版权所以本模板不能附带字体，请务必手动上传字体文件，并在各个专业模板下手动指定字体。
%           具体方法参照 GitHub 主页的说明。
%     \item 当前的Overleaf默认使用TexLive 2017进行编译，但一些伪粗体复制乱码的问题需要TexLive 2019版本来解决。
%           所以各位同学可以在Overleaf上编写论文时务必切换到TexLive 2019或更新版本来编译，以免产生查重相关问题。
%           具体说明参照 GitHub 主页。
% \end{itemize}


% \section{节标题}

% 我们可以用includegraphics来插入现有的jpg等格式的图片，
% 如\autoref{fig:zju-logo}所示。

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=.3\linewidth]{logo/zju}
%     \caption{\label{fig:zju-logo}浙江大学LOGO}
% \end{figure}


% \subsection{小节标题}


% \par 如\autoref{tab:sample}所示，这是一张自动调节列宽的表格。

% \begin{table}[htbp]
%     \caption{\label{tab:sample}自动调节列宽的表格}
%     \begin{tabularx}{\linewidth}{c|X<{\centering}}
%         \hline
%         第一列 & 第二列 \\ \hline
%         xxx & xxx \\ \hline
%         xxx & xxx \\ \hline
%         xxx & xxx \\ \hline
%     \end{tabularx}
% \end{table}


% \par 如\autoref{equ:sample}，这是一个公式

% \begin{equation}
%     \label{equ:sample}
%     A=\overbrace{(a+b+c)+\underbrace{i(d+e+f)}_{\text{虚数}}}^{\text{复数}}
% \end{equation}

% \chapter{另一章}


% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=.3\linewidth]{example-image-a}
%     \caption{\label{fig:fig-placeholder}图片占位符}
% \end{figure}

% \chapter{再一章}

% \par 如\autoref{alg:sample}，这是一个算法

% \begin{algorithm}[H]
%     \begin{algorithmic} % enter the algorithmic environment
%         \REQUIRE $n \geq 0 \vee x \neq 0$
%         \ENSURE $y = x^n$
%         \STATE $y \Leftarrow 1$
%         \IF{$n < 0$}
%         \STATE $X \Leftarrow 1 / x$
%         \STATE $N \Leftarrow -n$
%         \ELSE
%         \STATE $X \Leftarrow x$
%         \STATE $N \Leftarrow n$
%         \ENDIF
%         \WHILE{$N \neq 0$}
%         \IF{$N$ is even}
%         \STATE $X \Leftarrow X \times X$
%         \STATE $N \Leftarrow N / 2$
%         \ELSE[$N$ is odd]
%         \STATE $y \Leftarrow y \times X$
%         \STATE $N \Leftarrow N - 1$
%         \ENDIF
%         \ENDWHILE
%     \end{algorithmic}
%     \caption{\label{alg:sample}算法样例}
% \end{algorithm}
